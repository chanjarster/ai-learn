为什么要计算协方差矩阵的特征值和特征向量？PCA 背后的核心思想

## 为什么要使用协方差矩阵？

为什么我们要使用样本数据中，各个维度之间的协方差，来构建一个新的协方差矩阵？要弄清楚这一点，首先要回到 PCA 最终的目标：降维。降维就是要去除那些表达信息量少，或者冗余的维度。

**方差**

首先来看如何定义维度的信息量大小。

* 这里我们认为样本在某个特征上的差异就越大，那么这个特征包含的信息量就越大，就越重要。
* 相反，信息量就越小，需要被过滤掉。

很自然，我们就能想到使用某维特征的方差来定义样本在这个特征维度上的差异。

**皮尔森（Pearson）相关系数**

另一方面，我们要看如何发现冗余的信息。如果两种特征是有很高的相关性，那我们可以从一个维度的值推算出另一个维度的值，所表达的信息就是重复的。在实际运用中，我们可以使用**皮尔森（Pearson）相关系数**，来**描述两个变量之间的线性相关程度**。**这个系数的取值范围是 [−1,1]，绝对值越大，说明相关性越高，正数表示正相关，负数表示负相关。**

![](pca-why/formula-pearson.webp ':size=450')

其中 n 表示向量维度，xk,i 和 xk,j 分别为两个特征维度 i 和 j 在第 k 个采样上的数值。 x,iˉ 和 x,jˉ 分别表示两个特征维度上所有样本的均值，σx 和 σy 分别表示两个特征维度上所有样本的标准差。

**协方差**

我把皮尔森系数的公式稍加变化，你来观察一下皮尔森系数和协方差之间的关系。

![](pca-why/formula-pearson-cov.webp ':size=450')

你看，变换后的分子不就是协方差吗？而分母类似于标准化数据中的分母。所以在本质上，皮尔森相关系数和数据标准化后的协方差是一致的。

考虑到**协方差既可以衡量信息量的大小（主对角线），也可以衡量不同维度之间的相关性**，因此我们就使用各个维度之间的协方差所构成的矩阵，作为 PCA 分析的对象（分析对象是协方差矩阵）。

既然协方差矩阵提供了我们所需要的方差和相关性，那么下一步，我们就要考虑对这个矩阵进行怎样的操作了。

## 为什么要计算协方差矩阵的特征值和特征向量？

从两个角度来理解。

**第一个角度是对角矩阵。**

> 所谓对角矩阵，就是说只有矩阵主对角线之上的元素有非 0 值，而其他元素的值都为 0。

协方差矩阵的主对角线上，都是表示信息量的方差，而其他元素都是表示相关性的协方差。既然我们希望尽可能保留大信息量的维度，而去除相关的维度，那么就意味着我们希望对协方差进行对角化，尽可能地使得矩阵只有主对角线上有非 0 元素。

假如我们确实可以把矩阵尽可能地对角化，那么对角化之后的矩阵，它的主对角线上元素就是、或者接近矩阵的特征值，而特征值本身又表示了转换后的方差，也就是信息量。而此时，对应的各个特征向量之间是基本正交的，也就是相关性极低甚至没有相关性。

**第二个角度是特征值和特征向量的几何意义。**

在向量空间中，对某个向量左乘一个矩阵，实际上是对这个向量进行了一次变换。在这个变换的过程中，被左乘的向量主要发生旋转和伸缩这两种变化。如果左乘矩阵对某一个向量或某些向量只发生伸缩变换，不对这些向量产生旋转的效果，那么这些向量就称为这个矩阵的特征向量，而伸缩的比例就是特征值。换句话来说，某个矩阵的特征向量表示了这个矩阵在空间中的变换方向，这些方向都是趋于正交的，而特征值表示每个方向上伸缩的比例。

如果一个特征值很大，那么说明在对应的特征向量所表示的方向上，伸缩幅度很大。这也是为什么，我们需要使用原始的数据去左乘这个特征向量，来获取降维后的新数据。因为这样做可以帮助我们找到一个方向，让它最大程度地包含原有的信息。需要注意的是，**这个新的方向，往往不代表原始的特征，而是多个原始特征的组合和缩放**。


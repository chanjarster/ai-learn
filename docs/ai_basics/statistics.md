https://time.geekbang.org/column/article/1341

同线性代数一样，概率论也代表了一种看待世界的方式，**其关注的焦点是无处不在的可能性**。对随机事件发生的可能性进行规范的数学描述就是概率论的公理化过程。概率的公理化结构体现出的是对概率本质的一种认识。

## 频率学派和贝叶斯学派

### 频率学派

**“频率学派”（frequentist probability）**

从事件发生的频率认识概率的方法被称为“频率学派”（frequentist probability），频率学派口中的“概率”，其实是一个**可独立重复的随机实验**中**单个结果**出现频率的**极限**。

频率学派依赖的基础是古典概率模型。**在古典概率模型中，试验的结果只包含有限个基本事件，且每个基本事件发生的可能性相同**。如此一来，假设所有基本事件的数目为 n，待观察的随机事件 A 中包含的基本事件数目为 k，则古典概率模型下事件概率的计算公式为：

![](statistics/form-1.jpg ':size=100')

如果要刻画两个随机事件之间的关系，就需要引入条件概率。

**条件概率（conditional probability）**是根据已有信息对样本空间进行调整后得到的新的概率分布。假定有两个随机事件 A 和 B，条件概率就是指事件 A 在事件 B 已经发生的条件下发生的概率，用以下公式表示

![](statistics/form-2.jpg ':size=200')

上式中的 P(AB) 称为**联合概率（joint probability）**，表示的是 A 和 B 两个事件共同发生的概率。**如果联合概率等于两个事件各自概率的乘积，即 P(AB)=P(A)⋅P(B)，说明这两个事件的发生互不影响，即两者相互独立**。

**对于相互独立的事件，条件概率就是自身的概率，即 P(A｜B)=P(A)。**

基于条件概率可以得出**全概率公式（law of total probability）**。全概率公式的作用在于将复杂事件的概率求解转化为在不同情况下发生的简单事件的概率求和，即

![](statistics/form-3.jpg ':size=250')

![](statistics/form-4.jpg ':size=150')

全概率公式代表了频率学派解决概率问题的思路，即先做出一些假设（P(Bi)），再在这些假设下讨论随机事件的概率（P(A｜Bi)）。





### 贝叶斯学派

概率论的贝叶斯学派（Bayesian probability）其关注的焦点在于后验概率。

全概率公式稍作整理，就演化出了求解“逆概率”这一重要问题。所谓**“逆概率”**解决的是在事件结果已经确定的条件下（P(A)），推断各种假设发生的可能性（P(Bi｜A)），其通用的公式形式被称为贝叶斯公式：

![](statistics/form-5.jpg ':size=350')

[回顾这篇][1]文章关于分母的解释。

贝叶斯公式可以进一步抽象为贝叶斯定理（Bayes' theorem）：



![](statistics/form-6.jpg ':size=250')

式中的

* **P(H) 被称为先验概率（prior probability）**，即预先设定的假设成立的概率；
* **P(D｜H) 被称为似然概率（likelihood function）**，是在假设成立的前提下观测到结果的概率；
* **P(H｜D) 被称为后验概率（posterior probability）**，即在观测到结果的前提下假设成立的概率。

### 学派对比

**频率学派认为假设是客观存在且不会改变的，即存在固定的先验分布，只是作为观察者的我们无从知晓**。因而在计算具体事件的概率时，要先确定概率分布的类型和参数，以此为基础进行概率推演。

**贝叶斯学派则认为固定的先验分布是不存在的，参数本身也是随机数**。换言之，假设本身取决于观察结果，是不确定并且可以修正的。数据的作用就是对假设做出不断的修正，使观察者对概率的主观认识更加接近客观实际。

> 贝叶斯学派更符合实际，比如用户对商品的喜好程度，先验分布是不存在的，喜好程度是变化的。

## 概率的估计

多数（注意不是全部）机器学习模型采用的都是基于概率论的方法。但由于实际任务中可供使用的训练数据有限，因而需要对概率分布的参数进行估计。

概率的估计有两种方法：**最大似然估计法（maximum likelihood estimation）**和**最大后验概率法（maximum a posteriori estimation）**，两者分别体现出频率学派和贝叶斯学派对概率的理解方式。

**最大似然估计法（频率学派）**的思想是使训练数据出现的概率最大化，依此确定概率分布中的未知参数，估计出的概率分布也就最符合训练数据的分布。

**最大后验概率法（贝叶斯学派）**的思想则是根据训练数据和已知的其他条件，使未知参数出现的可能性最大化，并选取最可能的未知参数取值作为估计值。

## 随机变量分布

概率论的一个重要应用是描述**随机变量（random variable）**。根据取值空间的不同，随机变量可以分成两类：**离散型随机变量（discrete random variable）**和**连续型随机变量（continuous random variable）**。

离散变量的每个可能的取值都具有大于 0 的概率，取值和概率之间一一对应的关系就是**离散型随机变量**的分布律，也叫**概率质量函数（probability mass function）**。

概率质量函数在**连续型随机变量**上的**对应就是概率密度函数（probability density function）**。**概率密度函数体现的并非连续型随机变量的真实概率，而是不同取值可能性之间的相对关系。**对概率密度函数进行积分，得到的才是连续型随机变量的取值落在某个区间内的概率。

定义了概率质量函数与概率密度函数后，就可以给出一些重要分布的特性：

* 重要的**离散分布**包括**两点分布**、**二项分布**和**泊松分布**

* 重要的**连续分布**则包括**均匀分布**、**指数分布**和**正态分布**



**两点分布**又称伯努利分布（Bernoulli distribution）

见[这篇文章][2]复习



**二项分布（Binomial distribution）**

将满足参数为 p 的两点分布的随机试验独立重复 n 次，事件发生的次数即满足参数为 (n,p) 的二项分布。二项分布的表达式可以写成 ：

![](statistics/form-7.jpg ':size=350')

不太好理解，看[这篇文章][3]



**泊松分布（Poisson distribution）**

放射性物质在规定时间内释放出的粒子数所满足的分布，参数为 λ 的泊松分布表达式为 

![](statistics/form-8.jpg ':size=250')

当二项分布中的 n 很大且 p 很小时，其概率值可以由参数为 λ=np 的泊松分布的概率值近似。

不太好理解，看[这篇文章][4]



**均匀分布（uniform distribution）**

在区间 (a, b) 上满足均匀分布的连续型随机变量，其概率密度函数为 1 / (b - a)，这个变量落在区间 (a, b) 内任意等长度的子区间内的可能性是相同的。



**指数分布（exponential distribution）**

满足参数为 θ 指数分布的随机变量只能取正值，其概率密度函数为

![](statistics/form-9.jpg ':size=150')

指数分布的一个重要特征是无记忆性：即 P(X > s + t | X > s) = P(X > t)。

不太好理解，看[这篇文章][4]



**正态分布（normal distribution）**

参数为正态分布的概率密度函数为

![](statistics/form-10.jpg ':size=250')

**当 μ=0,σ=1 时，上式称为标准正态分布**。正态分布是最常见最重要的一种分布，自然界中的很多现象都近似地服从正态分布。

## 数字特征

另一类描述随机变量的参数是其数字特征。**数字特征是用于刻画随机变量某些特性的常数**，包括**数学期望（expected value）**、**方差（variance）**和**协方差（covariance）**。

**数学期望**

数学期望即均值，体现的是随机变量可能取值的加权平均，即根据每个取值出现的概率描述作为一个整体的随机变量的规律。

**方差**

方差表示的则是随机变量的取值与其数学期望的偏离程度。**方差较小意味着随机变量的取值集中在数学期望附近，方差较大则意味着随机变量的取值比较分散**。

**协方差**

协方差用于描述两个随机变量之间的相互关系。协方差度量了两个随机变量之间的线性相关性，即变量 Y 能否表示成以另一个变量 X 为自变量的 aX+b 的形式。

<font color="red">协方差刻画的都是线性相关的关系。如果随机变量之间的关系满足 Y=X<sup>2</sup>，这样的非线性相关性就超出了协方差的表达能力。_相关系数_同理。</font>

**相关系数**

根据协方差可以进一步求出**相关系数（correlation coefficient）**，相关系数是一个绝对值不大于 1 的常数，它等于 1 意味着两个随机变量满足完全正相关，等于 -1 意味着两者满足完全负相关，等于 0 则意味着两者不相关。

复习[PCA原理][5]来更好地理解**协方差**和**相关系数**。

[1]: /statistics/basic?id=贝叶斯定理
[2]: /statistics/distribution?id=伯努利分布
[3]: https://zhuanlan.zhihu.com/p/24692791
[4]: https://www.ruanyifeng.com/blog/2015/06/poisson-distribution.html
[5]: /linear_algebra/pca-why?id=为什么要使用协方差矩阵？

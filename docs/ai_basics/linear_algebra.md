https://time.geekbang.org/column/article/1340

## 几个名词

*标量*：由单独的数 a 构成的元素被称为标量（scalar），一个标量 a 可以是整数、实数或复数。

*向量*：如果多个标量 a1,a2,⋯,an 按一定顺序组成一个序列，这样的元素就被称为向量（vector）。

*矩阵*：如果将向量的所有标量都替换成相同规格的向量，得到的就是矩阵（matrix）。

*张量*：如果将矩阵中的每个标量元素再替换为向量的话，得到的就是张量（tensor）。直观地理解，张量就是高阶的矩阵。如果把三阶魔方的每一个小方块看作一个数，它就是个 3×3×3 的张量，3×3 的矩阵则恰是这个魔方的一个面，也就是张量的一个切片。

> 举例：在计算机存储中，标量占据的是零维数组；
>
> 向量占据的是一维数组，例如语音信号；
>
> 矩阵占据的是二维数组，例如灰度图像；
>
> 张量占据的是三维乃至更高维度的数组，例如 RGB 图像和视频。

## 向量

描述向量的数学语言：

* [范数（norm）][1]是对单个向量大小的度量，描述的是向量自身的性质，其作用是将向量映射为一个非负的数值。

* 内积（inner product）计算的则是两个向量之间的关系。两个相同维数向量内积即对应元素乘积的求和。内积能够表示两个向量之间的相对位置，即向量之间的夹角。如果内积为0，这意味着两个向量正交（orthogonality）。如果两个向量正交，说明他们线性无关，相互独立，互不影响。

  ![](linear_algebra/inner-product.jpg ':size=200')

## 线性空间

*线性空间*：如果有一个集合，它的元素都是具有相同维数的向量（可以是有限个或无限个）， 并且定义了加法和数乘等结构化的运算，这样的集合就被称为线性空间（linear space）

*内积空间*：定义了内积运算的线性空间则被称为内积空间（inner product space）。

**在线性空间中，任意一个向量代表的都是 n 维空间中的一个点；反过来， 空间中的任意点也都可以唯一地用一个向量表示。两者相互等效。**

*正交基*：在内积空间中，一组两两正交的向量构成这个空间的正交基（orthogonal basis），假若正交基中基向量的 L2 范数都是单位长度 1，这组正交基就是标准正交基（orthonormal basis）。正交基的作用就是给内积空间定义出经纬度。⼀旦描述内积空间的正交基确定了，向量和点之间的对应关系也就随之确定。

**内积空间的正交基并不唯一。**

## 矩阵

当作为参考系的标准正交基确定后，空间中的点就可以用向量表示。当这个点从一个位置移动到另一个位置时，描述它的向量也会发生改变。**点的变化对应着向量的线性变换（linear transformation），而描述对象变化抑或向量变换的数学语言，正是矩阵。**

在线性空间中，变化的实现有两种方式：

* 一是点本身的变化，使某个点发生变化的方法是用**代表变化的矩阵乘以代表对象的向量**。
* 二是参考系的变化，矩阵的作用就是对正交基进行变换。

对于矩阵和向量的相乘，就存在不同的解读方式：

![](linear_algebra/form-2.jpg ':size=150')

这个表达式既可以理解为向量 x 经过矩阵 A 所描述的变换，变成了向量 y；

也可以理解为一个对象在坐标系 A 的度量下得到的结果为向量 x，在标准坐标系 I（单位矩阵：主对角线元素为 1，其余元素为 0）的度量下得到的结果为向量 y。

**而对坐标系施加变换的方法，就是让表示原始坐标系的矩阵与表示变换的矩阵相乘。**

描述矩阵的⼀对重要参数是*特征值（eigenvalue）*和*特征向量（eigenvector）*。对于给定的矩阵 A，假设其特征值为λ，特征向量为 x，则它们之间的关系如下：

![](linear_algebra/form-3.jpg ':size=150')

矩阵代表了向量的变换，其效果通常是对原始向量同时施加方向变化和尺度变化。可对于有些特殊的向量，矩阵的作用只有尺度变化而没有方向变化，也就是只有伸缩的效果而没有旋转的效果。**对于给定的矩阵来说，这类特殊的向量就是矩阵的特征向量，特征向量的尺度变化系数就是特征值。**

**矩阵特征值和特征向量的动态意义在于表示了变化的速度和方向**。如果把矩阵所代表的变化看作奔跑的人，那么矩阵的特征值就代表了他奔跑的速度，特征向量代表了他奔跑的方向。但矩阵可不是普通人，它是三头六臂的哪吒，他的不同分身以不同速度（特征值）在不同方向（特征向量）上奔跑，所有分身的运动叠加在⼀起才是矩阵的效果。

**求解给定矩阵的特征值和特征向量的过程叫做特征值分解**，但能够进行特征值分解的矩阵必须是 n 维方阵。将特征值分解算法推广到所有矩阵之上，就是更加通用的[奇异值分解][2]。

[1]: linear_algebra/vector-space?id=向量的长度
[2]: https://time.geekbang.org/column/article/87657

